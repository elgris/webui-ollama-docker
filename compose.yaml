services:
  ollama:
    image: ollama/ollama:rocm
    container_name: ollama
    environment:
      OLLAMA_MODELS: /root/.ollama/models
      # Set for RX 6700s (gfx1032), adjust for your GPU.
      HSA_OVERRIDE_GFX_VERSION: "10.3.0"
      # Makes use of dGPU which goes as "0" on my laptop, YMMV.
      # Change to other indices to switch it to another GPU (e.g. "1" for iGPU).
      HIP_VISIBLE_DEVICES: "0"
    devices:
      - "/dev/kfd"
      - "/dev/dri"
    group_add:
      - video
    volumes:
      - $HOME/.ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      backend:
        ipv4_address: 10.1.0.2

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      # Change to `true` if you need auth (login, user registration, etc).
      WEBUI_AUTH: false
      # TODO: replace all of these with `OFFLINE_MODE: true`
      # once https://github.com/open-webui/open-webui/pull/7741
      # gets into the recent docker image.
      RAG_EMBEDDING_MODEL_AUTO_UPDATE: false
      RAG_RERANKING_MODEL_AUTO_UPDATE: false
      WHISPER_MODEL_AUTO_UPDATE: false
      OLLAMA_BASE_URL: http://10.1.0.2:11434
    volumes:
      - $HOME/.open-webui:/app/backend/data
    ports:
      - "3000:8080"
    networks:
      backend:
        ipv4_address: 10.1.0.3

networks:
  backend:
    driver: bridge
    ipam:
      config:
        - subnet: 10.1.0.0/16
          gateway: 10.1.0.1
